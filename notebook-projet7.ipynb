{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Création du dataset**","metadata":{}},{"cell_type":"markdown","source":"Notebook de base (non édité) : https://www.kaggle.com/code/hikmetsezen/micro-model-174-features-0-8-auc-on-home-credit","metadata":{}},{"cell_type":"markdown","source":"> **Imports**","metadata":{}},{"cell_type":"code","source":"!pip install ppscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T13:34:15.193961Z","iopub.execute_input":"2023-09-15T13:34:15.194516Z","iopub.status.idle":"2023-09-15T13:34:15.208585Z","shell.execute_reply.started":"2023-09-15T13:34:15.194480Z","shell.execute_reply":"2023-09-15T13:34:15.207643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/home-credit-default-risk/sample_submission.csv')\n\nprevious_application = pd.read_csv('/kaggle/input/home-credit-default-risk/previous_application.csv')\n\ninstallments_payments = pd.read_csv('/kaggle/input/home-credit-default-risk/installments_payments.csv')\n\ncredit_card_balance = pd.read_csv('/kaggle/input/home-credit-default-risk/credit_card_balance.csv')\n\nbureau_balance = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau_balance.csv')\n\nbureau = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau.csv')\n\napplication_train = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\n\napplication_test = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\n\nPOS_CASH_balance = pd.read_csv('/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:45:59.926375Z","iopub.execute_input":"2023-09-14T15:45:59.926823Z","iopub.status.idle":"2023-09-14T15:46:58.931568Z","shell.execute_reply.started":"2023-09-14T15:45:59.926789Z","shell.execute_reply":"2023-09-14T15:46:58.929817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = [application_train, application_test, bureau, bureau_balance, POS_CASH_balance, \n            credit_card_balance, installments_payments, previous_application, sample_submission]\n\ndataset_names = ['application_train', 'application_test', 'bureau', 'bureau_balance', \n                 'POS_CASH_balance', 'credit_card_balance', 'installments_payments', \n                 'previous_application', 'sample_submission']\n\ncolumns = []\nfor dataset in datasets :\n    columns += list(dataset.columns)\n\nunique_columns = []\n[unique_columns.append(column) for column in columns if column not in unique_columns]\nprint(f\"Nombre de features uniques : {len(unique_columns)}\")\nfor i in range(len(datasets)):\n    print(f\"\\nDimensions de {dataset_names[i]} : {datasets[i].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:46:58.934764Z","iopub.execute_input":"2023-09-14T15:46:58.935188Z","iopub.status.idle":"2023-09-14T15:46:58.944751Z","shell.execute_reply.started":"2023-09-14T15:46:58.935154Z","shell.execute_reply":"2023-09-14T15:46:58.943775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On doit se retrouver avec un dataset d'entraînement de taille 307511 * n\n\net un dataset de test de taille 48744 * n avec n <= 195.","metadata":{}},{"cell_type":"markdown","source":"> **JOINTURES**","metadata":{}},{"cell_type":"code","source":"# Descriptions des variables des fichiers précédents : uilisation pour trouver les clés de jointure entre les fichiers\ndesc = pd.read_csv('/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv', encoding=\"ISO-8859-1\")\ndesc.loc[desc.Row.duplicated(), ['Table', 'Row', 'Description']].sort_values(by='Row')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:46:58.946021Z","iopub.execute_input":"2023-09-14T15:46:58.947670Z","iopub.status.idle":"2023-09-14T15:46:58.990269Z","shell.execute_reply.started":"2023-09-14T15:46:58.947626Z","shell.execute_reply":"2023-09-14T15:46:58.989143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut joindre les fichiers grâce aux clés d'identification : SK_ID_CURR, SK_ID_BUREAU, SK_ID_PREV.\n\nLe fichier sample_applications ne sera pas utilisé dans la jointure des fichiers.","metadata":{}},{"cell_type":"code","source":"train_ids = application_train['SK_ID_CURR']\ntest_ids = application_test['SK_ID_CURR']\n\nlabels = application_train['TARGET']\n\ndef merge_data():\n    prev = credit_card_balance.merge(installments_payments, how='inner', on='SK_ID_PREV')\n    prev = prev.merge(POS_CASH_balance, how='inner', on='SK_ID_PREV')\n    prev = prev.merge(previous_application, how='inner', on='SK_ID_PREV')\n    \n    bur = bureau.merge(bureau_balance, how='inner', on='SK_ID_BUREAU')\n    return prev, bur","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:46:58.992012Z","iopub.execute_input":"2023-09-14T15:46:58.992350Z","iopub.status.idle":"2023-09-14T15:46:59.000204Z","shell.execute_reply.started":"2023-09-14T15:46:58.992320Z","shell.execute_reply":"2023-09-14T15:46:58.999117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nprev, bur = merge_data()\nprint(prev.shape, bur.shape)\n'''\n# Prends trop de RAM","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:46:59.001509Z","iopub.execute_input":"2023-09-14T15:46:59.002124Z","iopub.status.idle":"2023-09-14T15:46:59.017594Z","shell.execute_reply.started":"2023-09-14T15:46:59.002088Z","shell.execute_reply":"2023-09-14T15:46:59.016199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A cause des duplicatas de la variable SK_ID_CURR de bureau, une jointure des fichiers modifie le nombre de demandes dans le dataframe 'applications', il y a beaucoup trop de lignes à stocker.\n\nOn abandonne la jointure directe : on prend les deux datasets application_train/test et on rajoute des features avec le reste des fichiers via la méthode .groupby().","metadata":{}},{"cell_type":"code","source":"# Groupby pour bureau/bureau_balance\n\nohe = OneHotEncoder(sparse_output=False)\narray_status = np.reshape(np.array(bureau['CREDIT_ACTIVE']), (-1,1))\nohe_status = pd.DataFrame(ohe.fit_transform(array_status),\n                          columns=list(pd.Series(ohe.get_feature_names_out()).str.split('_',\n                                                 expand=True)[1].values))\nbureau.reset_index(drop=True, inplace=True)\nfor col in ohe_status.columns:\n    bureau = pd.concat([bureau, ohe_status.loc[:, col]], axis=1)\n    \ndef bureau_groupby_merge(data):\n    application = pd.DataFrame()\n    application['SK_ID_CURR'] = pd.Series(bureau['SK_ID_CURR'].unique()).sort_values()\n    \n    application['NB_CREDIT_ACTIVE'] = bureau.groupby('SK_ID_CURR')['Active'].sum().values\n    application['NB_CREDIT_DEBT'] = bureau.groupby('SK_ID_CURR')['Bad debt'].sum().values\n    application['NB_CREDIT_CLOSED'] = bureau.groupby('SK_ID_CURR')['Closed'].sum().values\n    application['NB_CREDIT_SOLD'] = bureau.groupby('SK_ID_CURR')['Sold'].sum().values\n    application['MEAN_DAYS_BETWEEN_APP'] = bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].mean().values\n    application['MEAN_CREDIT_DAY_OVERDUE'] = bureau.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].mean().values\n    application['REMAINING_CREDIT_DAYS'] = bureau.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].max().values\n    application['MAX_OVERDUE'] = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].max().values\n    application['NB_PROLONGATIONS'] = bureau.groupby('SK_ID_CURR')['CNT_CREDIT_PROLONG'].sum().values\n    application['TOT_CREDIT_AMT'] = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum().values\n    application['TOT_CREDIT_DEBT'] = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum().values\n    application['MAX_CREDIT_LIMIT'] = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].max().values\n    application['TOT_CREDIT_OVERDUE'] = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].sum().values\n    application['NB_CREDIT_TYPES'] = bureau.groupby('SK_ID_CURR')['CREDIT_TYPE'].nunique().values\n    application['MEAN_DAYS_SINCE_UPDATE'] = bureau.groupby('SK_ID_CURR')['DAYS_CREDIT_UPDATE'].mean().values\n    application['TOT_ANNUITY'] = bureau.groupby('SK_ID_CURR')['AMT_ANNUITY'].sum().values\n    \n    merged = data.merge(application, how='left', on='SK_ID_CURR')\n    return merged\n\ndata_train = bureau_groupby_merge(application_train)\ndata_test = bureau_groupby_merge(application_test)\n\ndel bureau\ndel bureau_balance\ndel application_train\ndel application_test\n\nprint(data_train.shape)\nprint(data_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:46:59.019359Z","iopub.execute_input":"2023-09-14T15:46:59.020027Z","iopub.status.idle":"2023-09-14T15:47:06.786676Z","shell.execute_reply.started":"2023-09-14T15:46:59.019990Z","shell.execute_reply":"2023-09-14T15:47:06.785261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Groupby pour previous_application\n\nohe = OneHotEncoder(sparse_output=False)\narray_status = np.reshape(np.array(previous_application['NAME_CONTRACT_STATUS']), (-1,1))\nohe_status = pd.DataFrame(ohe.fit_transform(array_status),\n                          columns=list(pd.Series(ohe.get_feature_names_out()).str.split('_',\n                                                 expand=True)[1].values))\nprevious_application.reset_index(drop=True, inplace=True)\nfor col in ohe_status.columns:\n    previous_application = pd.concat([previous_application, ohe_status.loc[:, col]], axis=1)\n\ndef previous_groupby_merge(data):\n    application = pd.DataFrame()\n    application['SK_ID_CURR'] = pd.Series(previous_application['SK_ID_CURR'].unique()).sort_values()\n    \n    application['NB_PREV_CONTRACT_TYPE'] = previous_application.groupby('SK_ID_CURR')['NAME_CONTRACT_TYPE'].nunique().values\n    application['MEAN_PREV_ANNUITY'] = previous_application.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean().values\n    application['MEAN_PREV_APP'] = previous_application.groupby('SK_ID_CURR')['AMT_APPLICATION'].mean().values\n    application['MEAN_PREV_CREDIT'] = previous_application.groupby('SK_ID_CURR')['AMT_CREDIT'].mean().values\n    application['MEAN_PREV_DOWNPAYMENT'] = previous_application.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].mean().values\n    application['MEAN_PREV_GOODSPRICE'] = previous_application.groupby('SK_ID_CURR')['AMT_GOODS_PRICE'].mean().values\n    application['NB_PREV_APPROVED'] = previous_application.groupby('SK_ID_CURR')['Approved'].count().values\n    application['NB_PREV_CANCELED'] = previous_application.groupby('SK_ID_CURR')['Canceled'].count().values\n    application['NB_PREV_REFUSED'] = previous_application.groupby('SK_ID_CURR')['Refused'].count().values\n    application['NB_PREV_UNUSED'] = previous_application.groupby('SK_ID_CURR')['Unused offer'].count().values\n    application['NB_PREV_GOODSTYPE'] = previous_application.groupby('SK_ID_CURR')['NAME_GOODS_CATEGORY'].count().values\n    application['DAYS_FIRST_DRAWING'] = previous_application.groupby('SK_ID_CURR')['DAYS_FIRST_DRAWING'].max().values\n    application['DAYS_FIRST_DUE'] = previous_application.groupby('SK_ID_CURR')['DAYS_FIRST_DUE'].min().values\n    application['NB_PREV_INSURED'] = previous_application.groupby('SK_ID_CURR')['NFLAG_INSURED_ON_APPROVAL'].count().values\n    \n    merged = data.merge(application, how='left', on='SK_ID_CURR')\n    return merged\n\ndata_train = previous_groupby_merge(data_train)\ndata_test = previous_groupby_merge(data_test)\n\ndel previous_application\n\nprint(data_train.shape)\nprint(data_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:06.789177Z","iopub.execute_input":"2023-09-14T15:47:06.789686Z","iopub.status.idle":"2023-09-14T15:47:16.781418Z","shell.execute_reply.started":"2023-09-14T15:47:06.789639Z","shell.execute_reply":"2023-09-14T15:47:16.780258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Groupby pour POS_CASH_balance\n\ndef POS_CASH_groupby_merge(data):\n    application = pd.DataFrame()\n    application['SK_ID_CURR'] = pd.Series(POS_CASH_balance['SK_ID_CURR'].unique()).sort_values()\n    \n    application['MEAN_INSTALMENTS'] = POS_CASH_balance.groupby('SK_ID_CURR')['CNT_INSTALMENT'].mean().values\n    application['MAX_INSTALMENTS_LEFT'] = POS_CASH_balance.groupby('SK_ID_CURR')['CNT_INSTALMENT_FUTURE'].max().values\n    application['MEAN_DAYS_PAST_DUE'] = POS_CASH_balance.groupby('SK_ID_CURR')['SK_DPD'].mean().values\n    \n    merged = data.merge(application, how='left', on='SK_ID_CURR')\n    return merged\n\ndata_train = POS_CASH_groupby_merge(data_train)\ndata_test = POS_CASH_groupby_merge(data_test)\n\ndel POS_CASH_balance\n\nprint(data_train.shape)\nprint(data_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:16.786412Z","iopub.execute_input":"2023-09-14T15:47:16.786805Z","iopub.status.idle":"2023-09-14T15:47:22.463537Z","shell.execute_reply.started":"2023-09-14T15:47:16.786774Z","shell.execute_reply":"2023-09-14T15:47:22.462247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Groupby pour credit_card_balance\n\ndef credit_card_groupby_merge(data):\n    application = pd.DataFrame()\n    application['SK_ID_CURR'] = pd.Series(credit_card_balance['SK_ID_CURR'].unique()).sort_values()\n    \n    application['MEAN_BALANCE'] = credit_card_balance.groupby('SK_ID_CURR')['AMT_BALANCE'].mean().values\n    application['MAX_CREDIT_LIMIT'] = credit_card_balance.groupby('SK_ID_CURR')['AMT_CREDIT_LIMIT_ACTUAL'].max().values\n    application['MEAN_DRAWING'] = credit_card_balance.groupby('SK_ID_CURR')['AMT_DRAWINGS_CURRENT'].mean().values\n    application['MIN_INSTALMENT'] = credit_card_balance.groupby('SK_ID_CURR')['AMT_INST_MIN_REGULARITY'].min().values\n    application['MEAN_MONTHLY_PAYMENT'] = credit_card_balance.groupby('SK_ID_CURR')['AMT_PAYMENT_TOTAL_CURRENT'].mean().values\n    application['MEAN_DRAWING_FREQUENCY'] = credit_card_balance.groupby('SK_ID_CURR')['CNT_DRAWINGS_CURRENT'].mean().values\n    \n    merged = data.merge(application, how='left', on='SK_ID_CURR')\n    return merged\n\ndata_train = credit_card_groupby_merge(data_train)\ndata_test = credit_card_groupby_merge(data_test)\n\ndel credit_card_balance\n\nprint(data_train.shape)\nprint(data_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:22.465263Z","iopub.execute_input":"2023-09-14T15:47:22.465641Z","iopub.status.idle":"2023-09-14T15:47:25.724967Z","shell.execute_reply.started":"2023-09-14T15:47:22.465609Z","shell.execute_reply":"2023-09-14T15:47:25.723717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pas de feature pertinente à extraire\ndel installments_payments","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:25.726153Z","iopub.execute_input":"2023-09-14T15:47:25.726487Z","iopub.status.idle":"2023-09-14T15:47:25.732141Z","shell.execute_reply.started":"2023-09-14T15:47:25.726457Z","shell.execute_reply":"2023-09-14T15:47:25.730612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fin de la jointure. Nos datasets finaux sont data_train et data_test.","metadata":{}},{"cell_type":"markdown","source":"# **Nettoyage des données**","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\ncolors = sns.color_palette('pastel')[0:data_train.dtypes.nunique()]\ncounts = data_train.dtypes.value_counts()\naxs[0].pie(counts.values, labels=counts.index, colors=colors,\n        autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p, p*sum(counts.values)/100))\naxs[0].set_title('Data_train dtypes')\n\ncolors = sns.color_palette('pastel')[0:data_test.dtypes.nunique()]\ncounts = data_test.dtypes.value_counts()\naxs[1].pie(counts.values, labels=counts.index, colors=colors,\n           autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p, p*sum(counts.values)/100))\naxs[1].set_title('Data_test dtypes')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:25.734401Z","iopub.execute_input":"2023-09-14T15:47:25.734823Z","iopub.status.idle":"2023-09-14T15:47:26.090720Z","shell.execute_reply.started":"2023-09-14T15:47:25.734788Z","shell.execute_reply":"2023-09-14T15:47:26.089431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"16 variables à encoder si l'étude des valeurs manquantes ne nous amène pas à supprimer des variables catégoriques.","metadata":{}},{"cell_type":"code","source":"int64_col = data_train.dtypes[data_train.dtypes=='int64'].index\ndescribe = data_train[int64_col].describe()\ndescribe.iloc[:, :20]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:26.092251Z","iopub.execute_input":"2023-09-14T15:47:26.092646Z","iopub.status.idle":"2023-09-14T15:47:26.868738Z","shell.execute_reply.started":"2023-09-14T15:47:26.092613Z","shell.execute_reply":"2023-09-14T15:47:26.867866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"describe.iloc[:, 20:]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:26.870253Z","iopub.execute_input":"2023-09-14T15:47:26.870586Z","iopub.status.idle":"2023-09-14T15:47:26.902297Z","shell.execute_reply.started":"2023-09-14T15:47:26.870558Z","shell.execute_reply":"2023-09-14T15:47:26.901044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les variables TARGET, les 6 variabes FLAG_, REG_REGION_NOT_LIVE_REGION, REG_REGION_NOT_WORK_REGION, \nLIVE_REGION_NOT_WORK_REGION, REG_CITY_NOT_LIVE_CITY, REG_CITY_NOT_WORK_CITY, LIVE_CITY_NOT_WORK_CITY et les 20 variables FLAG_DOCUMENT_ sont des variables catégoriques (binaires).","metadata":{}},{"cell_type":"code","source":"train_cat = list(data_train.dtypes[data_train.dtypes == object].index)\ntrain_cat += ['TARGET', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n              'FLAG_PHONE', 'FLAG_EMAIL', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n              'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',\n              'LIVE_CITY_NOT_WORK_CITY', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', \n              'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', \n              'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', \n              'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16',\n              'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n              'FLAG_DOCUMENT_21']\n\ntrain_num = [col for col in data_train.columns if col not in train_cat]\n\nprint(f\"Nombre de variables catégoriques : {len(train_cat)}\")\nprint(f\"Nombre de variables numériques : {len(train_num)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:26.904519Z","iopub.execute_input":"2023-09-14T15:47:26.905591Z","iopub.status.idle":"2023-09-14T15:47:26.916484Z","shell.execute_reply.started":"2023-09-14T15:47:26.905544Z","shell.execute_reply":"2023-09-14T15:47:26.915376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAG_DOCUMENT=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', \n               'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', \n               'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', \n               'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', \n               'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', \n               'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', \n               'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n\ndef feature_engineering(data):\n    data['NB_FLAG_DOCUMENTS'] = data[FLAG_DOCUMENT].sum(axis=1)\n    data.drop(FLAG_DOCUMENT,axis=1,inplace=True)\n    \n    data['AGE']=data['DAYS_BIRTH']/(-365)\n\n    data['CREDIT_INCOME_RATIO'] = data['AMT_CREDIT'] / data['AMT_INCOME_TOTAL']\n    \nfeature_engineering(data_train)\nfeature_engineering(data_test)\n\ntrain_num += ['NB_FLAG_DOCUMENTS', 'AGE', 'CREDIT_INCOME_RATIO']\nfor cat in FLAG_DOCUMENT:\n    train_cat.remove(cat)\n\nprint(f\"Nombre de variables numériques : {len(train_num)}\\nNombre de variables catégoriques : {len(train_cat)}\")\nprint(\"Nouvelles dimensions\")\nprint(f\"data_train : {data_train.shape}\\ndata_test  {data_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:26.917981Z","iopub.execute_input":"2023-09-14T15:47:26.918315Z","iopub.status.idle":"2023-09-14T15:47:27.171722Z","shell.execute_reply.started":"2023-09-14T15:47:26.918287Z","shell.execute_reply":"2023-09-14T15:47:27.170510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Valeurs manquantes**","metadata":{}},{"cell_type":"code","source":"def missing_pct(data):\n    # Supprimer les variables vides à 60% au moins\n    shape_init = data.shape[1]\n    for col in data.columns:\n        if round(data[col].isna().sum()/data.shape[0]*100, 2) > 60:\n            data.drop(columns=[col], inplace=True)\n            if col in train_num:\n                train_num.remove(col)\n            elif col in train_cat:\n                train_cat.remove(col)\n    print(f\"Nombre de variables supprimées : {shape_init-data.shape[1]}\")\n    print(f\"Variables catégoriques restantes : {len(train_cat)}\")     \n    print(f\"Variables numériques restantes : {len(train_num)}\")\n    # Plot missing values\n    plt.figure(figsize=(15, 15))\n    counts = pd.Series(data.isna().sum()).loc[data.isna().sum()>0].sort_values()\n    print(f\"Nombre de variables avec des valeurs manquantes : {counts.shape[0]}\")\n    ax = sns.barplot(x=np.round(counts.values/data.shape[0]*100, 2), y=counts.index)\n    # Plot with % labels\n    for i in ax.containers:\n        plt.bar_label(i, )\n    plt.show()\n\nmissing_pct(data_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:27.173487Z","iopub.execute_input":"2023-09-14T15:47:27.174028Z","iopub.status.idle":"2023-09-14T15:47:33.841950Z","shell.execute_reply.started":"2023-09-14T15:47:27.173970Z","shell.execute_reply":"2023-09-14T15:47:33.840672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_pct(data_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:33.843802Z","iopub.execute_input":"2023-09-14T15:47:33.844639Z","iopub.status.idle":"2023-09-14T15:47:36.059414Z","shell.execute_reply.started":"2023-09-14T15:47:33.844595Z","shell.execute_reply":"2023-09-14T15:47:36.058073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Il y a une grande portion des datasets qui est vide : 83 variables en entraînement et 80 en test ont des valeurs manquantes.\nLa majorité de ces variables sont à moitié vides.\n\nIl y avait 23 variables dans chaque dataset avec plus de 60% de valeurs manquantes qui ont été supprimées.\n\nOn choisi d'abord d'utiliser un SimpleImputer avec moyenne pour les float et médiane pour les entiers.\n\nOn se réserve l'option d'y revenir si les résultats du classifieur ne sont pas satisfaisants.","metadata":{}},{"cell_type":"code","source":"# Stratégie = moyenne pour floats\nsi = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nfloat_64 = list(data_train.dtypes[data_train.dtypes == float].index)\ndata_train[float_64] = si.fit_transform(data_train[float_64])\nfloat_64 = list(data_test.dtypes[data_test.dtypes == float].index)\ndata_test[float_64] = si.fit_transform(data_test[float_64])\n\n# Stratégie = médiane pour ints\nsi = SimpleImputer(missing_values=np.nan, strategy='median')\n\ndata = data_train.drop(columns=['TARGET'])\nint_64 = list(data.dtypes[data.dtypes == int].index)\ndata_train[int_64] = si.fit_transform(data_train[int_64])\nint_64 = list(data_test.dtypes[data_test.dtypes == int].index)\ndata_test[int_64] = si.fit_transform(data_test[int_64])\n\nmissing_pct(data_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:36.060907Z","iopub.execute_input":"2023-09-14T15:47:36.061263Z","iopub.status.idle":"2023-09-14T15:47:44.089363Z","shell.execute_reply.started":"2023-09-14T15:47:36.061233Z","shell.execute_reply":"2023-09-14T15:47:44.088314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_pct(data_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:44.090669Z","iopub.execute_input":"2023-09-14T15:47:44.091726Z","iopub.status.idle":"2023-09-14T15:47:44.768242Z","shell.execute_reply.started":"2023-09-14T15:47:44.091674Z","shell.execute_reply":"2023-09-14T15:47:44.767123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fillna_cat(data):\n    # np.NaN dans NAME_TYPE_SUITE = pas d'accompagnateur\n    data['NAME_TYPE_SUITE'].fillna('Unaccompanied', inplace=True)\n    # np.NaN dans OCCUPATION_TYPE = autre occupation OU pas d'occupation\n    data['OCCUPATION_TYPE'].fillna('Other/No occupation', inplace=True)\n    # EMERGENCYSTATE_MODE : 'Yes' ou 'No' : Remplacer par le plus fréquent\n    data['EMERGENCYSTATE_MODE'].fillna(data['EMERGENCYSTATE_MODE'].value_counts().index[0], inplace=True)\n    # np.NaN dans HOUSETYPE_MODE = autre type de logement OU pas de logement\n    data['HOUSETYPE_MODE'].fillna('Other/No housetype', inplace=True)\n    # np.NaN dans WALLSMATERIAL_MODE = autre type de matériaux : catégorie existante\n    data['WALLSMATERIAL_MODE'].fillna('Others', inplace=True)\n\nfillna_cat(data_train)\nfillna_cat(data_test)\n\nprint(\"Nombre de valeurs manquantes\")\nprint(f\"- dans data_train : {data_train.isna().sum().sum()}\")\nprint(f\"- dans data_test : {data_test.isna().sum().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:44.769662Z","iopub.execute_input":"2023-09-14T15:47:44.770124Z","iopub.status.idle":"2023-09-14T15:47:45.754223Z","shell.execute_reply.started":"2023-09-14T15:47:44.770092Z","shell.execute_reply":"2023-09-14T15:47:45.752774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **DATA ANALYSIS**","metadata":{}},{"cell_type":"code","source":"counts = labels.value_counts()\n\nplt.pie(counts.values, labels=counts.index,\n        autopct=lambda p :'{:.2f}%  ({:,.0f})'.format(p, p*sum(counts.values)/100))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:45.755785Z","iopub.execute_input":"2023-09-14T15:47:45.756588Z","iopub.status.idle":"2023-09-14T15:47:45.934484Z","shell.execute_reply.started":"2023-09-14T15:47:45.756552Z","shell.execute_reply":"2023-09-14T15:47:45.933499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rappel :\n\n0 -> Crédit remboursé dans les temps\n\n1 -> Défaut de paiement\n\nLa variable cible est très déséquilibrée","metadata":{}},{"cell_type":"markdown","source":"Pour analyser les corrélations on encode toutes les variables de type objet.\n\nPour minimiser le nombre de colonnes ajoutées :\n\n- Si le nombre de catégories est inférieur ou égal à 2 : LabelEncoder\n- Sinon : OneHotEncoder","metadata":{}},{"cell_type":"code","source":"data_train = data_train.loc[data_train.CODE_GENDER != 'XNA', :]\nle = LabelEncoder()\n\ndef encoding(data):\n    original_columns = list(data.columns)\n    inf2 = [cat for cat in data.dtypes[data.dtypes==object].index if data[cat].nunique() <= 2]\n    sup2 = [cat for cat in data.dtypes[data.dtypes==object].index if data[cat].nunique() > 2]\n    \n    for cat in inf2:\n        data[cat] = le.fit_transform(data[cat])\n        \n    data = pd.get_dummies(data, columns= sup2)\n        \n    new_columns = [c for c in data.columns if c not in original_columns]\n    return data, new_columns\n\ndata_train, new_columns_tr = encoding(data_train)\ndata_test, new_columns_te = encoding(data_test)\n\nprint(f\"Nombre de colonnes ajoutées à data_train : {len(new_columns_tr)}\")\nprint(f\"Dimensions data_train : {data_train.shape}\")\nprint(f\"Nombre de colonnes ajoutées à data_test : {len(new_columns_te)}\")\nprint(f\"Dimensions data_test : {data_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:45.936051Z","iopub.execute_input":"2023-09-14T15:47:45.936390Z","iopub.status.idle":"2023-09-14T15:47:48.520210Z","shell.execute_reply.started":"2023-09-14T15:47:45.936353Z","shell.execute_reply":"2023-09-14T15:47:48.518786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Variables présentes dans le dataset d'entraînement et non dans le dataset de test :\")\nprint()\n\nvar_to_pop = [var for var in data_train.columns if var not in data_test.columns]\ndata_train.drop(columns=var_to_pop[1:], inplace=True)\n\nprint(var_to_pop)\nprint(f\"Dimensions data_train : {data_train.shape}\")\nprint(f\"Dimensions data_test : {data_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:47:48.521815Z","iopub.execute_input":"2023-09-14T15:47:48.522215Z","iopub.status.idle":"2023-09-14T15:47:48.642783Z","shell.execute_reply.started":"2023-09-14T15:47:48.522182Z","shell.execute_reply":"2023-09-14T15:47:48.641590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FINAL : 235 features avec variable d'identification.","metadata":{}},{"cell_type":"code","source":"pearson = data_train[['TARGET'] + [col for col in data_train.columns if col != 'TARGET']].corr()\n\npearson.style.format(\"{:.1}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T10:25:25.215594Z","iopub.execute_input":"2023-09-14T10:25:25.215959Z","iopub.status.idle":"2023-09-14T10:26:17.503902Z","shell.execute_reply.started":"2023-09-14T10:25:25.215928Z","shell.execute_reply":"2023-09-14T10:26:17.501988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les 3 variables EXT_SOURCE_ sont les seules à avoir un coefficient de corrélation absolu supérieur ou égal à 0.1 avec TARGET.\n\nGlobalement, la variable TARGET n'a pas l'air d'être influencée par les variables du dataset d'entraînement.\n\nDe plus, ces coeffcients nous montrent qu'il n'y a pas de data leakage.","metadata":{}},{"cell_type":"code","source":"cat_test = [col for col in data_test.columns if np.max(data_test[col])==1]\nnum = [col for col in data_test.columns if col not in cat]\nnum.remove('SK_ID_CURR')\ndata_test_sc = data_test.copy()\ndata_test_sc[num] = RobustScaler().fit_transform(data_test_sc[num])\n\ndata_test_sc['SK_ID_CURR'] = data_test_sc['SK_ID_CURR'].astype(int)\ndata_test_sc[cat_test] = data_test_sc[cat_test].astype(int)\n\ndata_test_sc.to_csv(r'app_test.csv', index=False) # upload données clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = [col for col in data_train.columns if np.max(data_train[col])==1]\nnum = [col for col in data_train.columns if col not in cat]\nnum.remove('SK_ID_CURR')\ndata_train_sc = data_train.copy()\ndata_train_sc[num] = RobustScaler().fit_transform(data_train_sc[num])\n\ndata_train_sc['SK_ID_CURR'] = data_train_sc['SK_ID_CURR'].astype(int)\ndata_train_sc[cat] = data_train_sc[cat].astype(int)\n\ndata_train_sc.to_csv(r'data_train.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Feature importance**","metadata":{}},{"cell_type":"code","source":"import ppscore as pps\ndata_train = pd.read_csv('data_train.csv')\ndata_train['TARGET'] = data_train['TARGET'].astype(object)\n\npredictors = pps.predictors(data_train, 'TARGET')\ndata_train['TARGET'] = data_train['TARGET'].astype(int)\npredictors = predictors.loc[predictors['ppscore'] > 0].sort_values(by='ppscore', ascending = False)\npredictors","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:59:25.984381Z","iopub.execute_input":"2023-09-15T10:59:25.984760Z","iopub.status.idle":"2023-09-15T10:59:58.841946Z","shell.execute_reply.started":"2023-09-15T10:59:25.984724Z","shell.execute_reply":"2023-09-15T10:59:58.840920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictive_var = list(predictors.loc[:, 'x'].values)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:59:58.843639Z","iopub.execute_input":"2023-09-15T10:59:58.843984Z","iopub.status.idle":"2023-09-15T10:59:58.848407Z","shell.execute_reply.started":"2023-09-15T10:59:58.843952Z","shell.execute_reply":"2023-09-15T10:59:58.847433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"176 variables sur 235 ont un Predictive Power Score non nul. Il sera nécessaire d'essayer un modèle avec pénalisation et de faire une étude de Feature Importance pour faire une sélection de feature si le modèle performe mal.\n\nAucun de ces score ne se dépasse 0.02 sachant qu'un ppscore parfait est égal à 1.","metadata":{}},{"cell_type":"markdown","source":"# **Modélisation**","metadata":{}},{"cell_type":"code","source":"# !pip install mlflow : sur terminal\n!pip install xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n\n# Metrics adaptés au problème de déséquilibre des coûts\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, fbeta_score, recall_score, precision_score\n\n#Modèles\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\n\n#gestion du desiquilibre de TARGET\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\nfrom sklearn.preprocessing import RobustScaler\n\nimport shap\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:00:06.094011Z","iopub.execute_input":"2023-09-15T11:00:06.094628Z","iopub.status.idle":"2023-09-15T11:00:12.476069Z","shell.execute_reply.started":"2023-09-15T11:00:06.094596Z","shell.execute_reply":"2023-09-15T11:00:12.474850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Avec over sampling**","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv('data_train.csv')\n\nX = data_train.drop(columns=['TARGET', 'SK_ID_CURR']).copy()\ny = data_train['TARGET'].copy()\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n\n# Le nombre de crédits en défaut de paiement étant assez faible (25 000) : \n# on choisit une méthode d'over-sampling\nsmote = SMOTE()\nX_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n\nprint('y_train', Counter(y_train_over))\nprint('y_val', Counter(y_val))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:00:15.195841Z","iopub.execute_input":"2023-09-15T11:00:15.197193Z","iopub.status.idle":"2023-09-15T11:00:30.865108Z","shell.execute_reply.started":"2023-09-15T11:00:15.197149Z","shell.execute_reply":"2023-09-15T11:00:30.863872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Il n'y a plus de déséquilibre des classes mais il faudra faire attention à l'overfitting possible des futurs modèles (conséquence de l'over-sampling)\n\nOn commence par un DummyClassifier :","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndef make_model(classifier, classifier__param, list_param):\n    pipeline = Pipeline(steps = [['scaler', MinMaxScaler()],\n                                 ['classifier', classifier]])\n    \n    param_grid = { classifier__param : list_param }\n    grid_search = GridSearchCV(estimator=pipeline,\n                               param_grid=param_grid,\n                               scoring='roc_auc',\n                               cv=3,\n                               n_jobs=-1)\n\n    grid_search.fit(X_train, y_train)\n    print(\"Best params : \")\n    print(grid_search.best_params_)\n    return grid_search.best_estimator_\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp = pd.DataFrame(index = ['precision', 'recall', 'Score métier', 'AUC'])\n\ndef score_métier(model, model_str):\n    y_proba = model.predict_proba(X_val)[:, 1]\n    \n    # Seuil optimisé pour favoriser les prédictions positives qui ont un moindre coût \n    scores = [fbeta_score(y_val, pd.Series(y_proba).apply(lambda x: 1 if x > seuil else 0),\n                          beta=2) for seuil in np.linspace(0.1, 0.9, 17)]\n    seuil_opt = np.linspace(0.1, 0.9, 17)[scores.index(np.max(scores))]\n    y_pred = pd.Series(y_proba).apply(lambda x: 1 if x > seuil_opt else 0)\n    print(f\"Seuil optimal = {seuil_opt}\")\n    \n    conf_mat = pd.DataFrame(confusion_matrix(y_val, y_pred), \n                            columns = ['pred_0', 'pred_1'],\n                            index = ['true_0', 'true_1'])\n     \n    precision = round(precision_score(y_val, y_pred), 3)\n    recall = round(recall_score(y_val, y_pred), 3)# Recall => priorité sur précision\n    fbeta = round(fbeta_score(y_val, y_pred, beta=10), 3)# Coût FN = 10 * Coût FP\n    auc = round(roc_auc_score(y_val, y_proba), 3)\n    \n    print(f\"Precision score       : {precision}\")\n    print(f\"Recall score          : {recall}\")\n    print(f\"FBêta score           : {fbeta}\")\n    print(f\"ROC AUC score         : {auc}\")\n    \n    comp[model_str] = [precision, recall, fbeta, auc]\n    \n    return conf_mat","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:00:30.866967Z","iopub.execute_input":"2023-09-15T11:00:30.867328Z","iopub.status.idle":"2023-09-15T11:00:30.880453Z","shell.execute_reply.started":"2023-09-15T11:00:30.867296Z","shell.execute_reply":"2023-09-15T11:00:30.879461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dummy = make_model(DummyClassifier(), 'classifier__strategy', [\"most_frequent\", \"prior\", \"stratified\", \"uniform\"])\ndummy = DummyClassifier(strategy='stratified')\ndummy.fit(X_train_over, y_train_over)\n\nscore_métier(dummy, 'Baseline')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:23:30.224499Z","iopub.execute_input":"2023-09-13T07:23:30.225219Z","iopub.status.idle":"2023-09-13T07:23:31.331138Z","shell.execute_reply.started":"2023-09-13T07:23:30.225178Z","shell.execute_reply":"2023-09-13T07:23:31.329807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train_over, y_train_over)\n\nscore_métier(lr, 'Logistic Regression') #3min","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:23:31.333053Z","iopub.execute_input":"2023-09-13T07:23:31.333560Z","iopub.status.idle":"2023-09-13T07:29:37.852423Z","shell.execute_reply.started":"2023-09-13T07:23:31.333513Z","shell.execute_reply":"2023-09-13T07:29:37.850257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Il y a moins de faux négatifs et tous les metrics ont été améliorés par contre il n'y a pas convergence.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs=-1)\nrfc.fit(X_train_over, y_train_over)\n\nscore_métier(rfc, 'Random Forest') # 8min environ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T10:28:34.971255Z","iopub.execute_input":"2023-09-14T10:28:34.972215Z","iopub.status.idle":"2023-09-14T10:38:16.660532Z","shell.execute_reply.started":"2023-09-14T10:28:34.972166Z","shell.execute_reply":"2023-09-14T10:38:16.659339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgbc = XGBClassifier()\nxgbc.fit(X_train_over, y_train_over)\n\nscore_métier(xgbc, 'XGradient Boosting') # 20min...","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:38:10.804167Z","iopub.execute_input":"2023-09-13T07:38:10.804619Z","iopub.status.idle":"2023-09-13T07:54:54.701166Z","shell.execute_reply.started":"2023-09-13T07:38:10.804579Z","shell.execute_reply":"2023-09-13T07:54:54.699083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le nombre de faux négatifs à augmenté.","metadata":{}},{"cell_type":"code","source":"import re\nX_train_over_lgbm = X_train_over.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\nX_val_lgbm = X_val.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nlgbm = LGBMClassifier()\nlgbm.fit(X_train_over_lgbm, y_train_over, eval_set=[(X_train_over_lgbm, y_train_over), (X_val_lgbm, y_val)],\n         eval_metric='auc')\n\n\nscore_métier(lgbm, 'LGBM Classifier') ","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:12:38.595154Z","iopub.execute_input":"2023-09-15T11:12:38.595543Z","iopub.status.idle":"2023-09-15T11:13:00.435555Z","shell.execute_reply.started":"2023-09-15T11:12:38.595514Z","shell.execute_reply":"2023-09-15T11:13:00.434412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pareil que XGBClassifier.","metadata":{}},{"cell_type":"code","source":"comp","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:55:22.649212Z","iopub.execute_input":"2023-09-13T07:55:22.649658Z","iopub.status.idle":"2023-09-13T07:55:22.664209Z","shell.execute_reply.started":"2023-09-13T07:55:22.649626Z","shell.execute_reply":"2023-09-13T07:55:22.663175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(15, 10))\ni=0\nj=0\nfor index in comp.index:\n    if j>1:\n        i+=1\n        j=0\n    val = comp.loc[index, :].sort_values()\n    sns.barplot(x=val.values, y=val.index, ax=axs[i, j])\n    axs[i, j].set_title(f\"{index}\")\n    j+=1\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:55:22.665899Z","iopub.execute_input":"2023-09-13T07:55:22.666267Z","iopub.status.idle":"2023-09-13T07:55:23.503417Z","shell.execute_reply.started":"2023-09-13T07:55:22.666237Z","shell.execute_reply":"2023-09-13T07:55:23.502110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp = comp[['LGBM Classifier', 'Random Forest']] # LGBMClassifier au lieu de XGBClassifier car XGB beaucoup plus long","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:55:23.505556Z","iopub.execute_input":"2023-09-13T07:55:23.506024Z","iopub.status.idle":"2023-09-13T07:55:23.513053Z","shell.execute_reply.started":"2023-09-13T07:55:23.505980Z","shell.execute_reply":"2023-09-13T07:55:23.511805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Avec under sampling**","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n\n# on essaie une méthode d'under-sampling\nnearmiss = NearMiss()\nX_train_under, y_train_under = nearmiss.fit_resample(X_train, y_train)\n\nprint('y_train', Counter(y_train_under))\nprint('y_val', Counter(y_val))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:00:30.881643Z","iopub.execute_input":"2023-09-15T11:00:30.882040Z","iopub.status.idle":"2023-09-15T11:01:50.385044Z","shell.execute_reply.started":"2023-09-15T11:00:30.882013Z","shell.execute_reply":"2023-09-15T11:01:50.383777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_under = LogisticRegression(max_iter=1000)\nlr_under.fit(X_train_under, y_train_under)\n\nscore_métier(lr_under, 'LR under')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:56:44.111648Z","iopub.execute_input":"2023-09-13T07:56:44.112005Z","iopub.status.idle":"2023-09-13T07:57:22.243696Z","shell.execute_reply.started":"2023-09-13T07:56:44.111975Z","shell.execute_reply":"2023-09-13T07:57:22.242453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_under = RandomForestClassifier(n_jobs=-1)\nrf_under.fit(X_train_under, y_train_under)\n\nscore_métier(rf_under, 'RF under')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:57:22.245782Z","iopub.execute_input":"2023-09-13T07:57:22.246245Z","iopub.status.idle":"2023-09-13T07:57:48.080885Z","shell.execute_reply.started":"2023-09-13T07:57:22.246191Z","shell.execute_reply":"2023-09-13T07:57:48.079471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_under = XGBClassifier()\nxgb_under.fit(X_train_under, y_train_under)\n\nscore_métier(xgb_under, 'XGB under')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:57:48.082744Z","iopub.execute_input":"2023-09-13T07:57:48.083344Z","iopub.status.idle":"2023-09-13T07:58:49.493658Z","shell.execute_reply.started":"2023-09-13T07:57:48.083304Z","shell.execute_reply":"2023-09-13T07:58:49.491908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_under_lgbm = X_train_under.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nlgbm_under = LGBMClassifier()\nlgbm_under.fit(X_train_under_lgbm, y_train_under, eval_set=[(X_train_under_lgbm, y_train_under), (X_val_lgbm, y_val)],\n               eval_metric='auc')\n\nscore_métier(lgbm_under, 'LGBM under')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:58:49.496057Z","iopub.execute_input":"2023-09-13T07:58:49.496572Z","iopub.status.idle":"2023-09-13T07:58:55.648260Z","shell.execute_reply.started":"2023-09-13T07:58:49.496518Z","shell.execute_reply":"2023-09-13T07:58:55.647436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:58:55.649867Z","iopub.execute_input":"2023-09-13T07:58:55.650412Z","iopub.status.idle":"2023-09-13T07:58:55.663301Z","shell.execute_reply.started":"2023-09-13T07:58:55.650377Z","shell.execute_reply":"2023-09-13T07:58:55.662073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"L'approche under-sampling nous ramène à un roc_auc d'environ 0.5.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(15, 10))\ni=0\nj=0\nfor index in comp.index:\n    if j>1:\n        i+=1\n        j=0\n    val = comp.loc[index, :].sort_values()\n    sns.barplot(x=val.values, y=val.index, ax=axs[i, j])\n    axs[i, j].set_title(f\"{index}\")\n    j+=1\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T07:58:55.664658Z","iopub.execute_input":"2023-09-13T07:58:55.665172Z","iopub.status.idle":"2023-09-13T07:58:56.994714Z","shell.execute_reply.started":"2023-09-13T07:58:55.665143Z","shell.execute_reply":"2023-09-13T07:58:56.993460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On va essayer d'optimiser :\n- les hyperparamètres du LGBMClassifier pour améliorer le recall\n- les hyperparamètres du RandomForest pour améliorer le recall","metadata":{}},{"cell_type":"markdown","source":"> **Optimisation des modèle choisi**","metadata":{}},{"cell_type":"code","source":"param_dict = {'n_estimators': np.linspace(20, 200, 10).astype(int),\n              'max_depth': np.linspace(2, 20, 10).astype(int),\n              'min_samples_split': [2, 5, 10],\n              'min_samples_leaf': [1, 2, 4],}\n                \nrs = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1), \n                        param_distributions = param_dict,\n                        scoring = 'recall',\n                        cv = 3,\n                        verbose = 3,\n                        n_jobs = -1)\n                        \nrs.fit(X_train_over, y_train_over)\n\nprint(rs.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_opt = RandomForestClassifier(n_estimators=50, max_depth=12, n_jobs=-1)\nrfc_opt.fit(X_train_over, y_train_over)\n\nscore_métier(rfc_opt, 'RandomForest tuned') # 3min","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:01:50.389214Z","iopub.execute_input":"2023-09-15T11:01:50.389559Z","iopub.status.idle":"2023-09-15T11:04:01.438967Z","shell.execute_reply.started":"2023-09-15T11:01:50.389532Z","shell.execute_reply":"2023-09-15T11:04:01.437765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Training Accuracy : ',\n      accuracy_score(y_train_over, rfc_opt.predict(X_train_over))*100)\nprint('Validation Accuracy : ',\n      accuracy_score(y_val, rfc_opt.predict(X_val))*100) # Pas d'overfitting","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:20:42.467451Z","iopub.execute_input":"2023-09-15T10:20:42.467828Z","iopub.status.idle":"2023-09-15T10:20:49.059582Z","shell.execute_reply.started":"2023-09-15T10:20:42.467797Z","shell.execute_reply":"2023-09-15T10:20:49.058041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dict = {'uniform_drop': [True, False],\n              'xgboost_dart_mode': [True, False],\n              'n_estimators': np.linspace(20, 200, 10).astype(int),\n              'num_leaves': [2, 20, 200],\n              'min_data_in_leaf': [1, 10, 100, 1000],\n              'max_depth': np.linspace(1, 10, 10).astype(int)}\n    \nrs1 = RandomizedSearchCV(LGBMClassifier(), \n                         param_distributions = param_dict,\n                         scoring = 'recall',\n                         cv = 2,\n                         verbose = 3,\n                         n_jobs = -1)\n                        \nrs1.fit(X_train_over_lgbm, y_train_over)\n\nprint(rs1.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T10:43:08.533797Z","iopub.execute_input":"2023-09-14T10:43:08.534174Z","iopub.status.idle":"2023-09-14T10:47:33.995147Z","shell.execute_reply.started":"2023-09-14T10:43:08.534142Z","shell.execute_reply":"2023-09-14T10:47:33.993748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_opt = LGBMClassifier(xgboost_dart_mode = False, uniform_drop = False, num_leaves = 200,\n                          n_estimators=160, min_data_in_leaf = 1000, max_depth = 10)\nlgbm_opt.fit(X_train_over_lgbm, y_train_over)\n\nscore_métier(lgbm_opt, 'LGBM tuned') # 1min","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:13:00.437131Z","iopub.execute_input":"2023-09-15T11:13:00.437438Z","iopub.status.idle":"2023-09-15T11:13:28.749215Z","shell.execute_reply.started":"2023-09-15T11:13:00.437412Z","shell.execute_reply":"2023-09-15T11:13:28.748136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Accuracy : ',\n      accuracy_score(y_train_over, lgbm_opt.predict(X_train_over_lgbm))*100)\nprint('Validation Accuracy : ',\n      accuracy_score(y_val, lgbm_opt.predict(X_val_lgbm))*100) #Pas d'overfitting","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:23:09.401836Z","iopub.execute_input":"2023-09-15T10:23:09.402229Z","iopub.status.idle":"2023-09-15T10:23:14.240691Z","shell.execute_reply.started":"2023-09-15T10:23:09.402195Z","shell.execute_reply":"2023-09-15T10:23:14.238516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp = comp[['Random Forest', 'RandomForest tuned', \n             'LGBM Classifier', 'LGBM tuned']]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T11:03:32.906645Z","iopub.execute_input":"2023-09-14T11:03:32.907695Z","iopub.status.idle":"2023-09-14T11:03:32.913798Z","shell.execute_reply.started":"2023-09-14T11:03:32.907658Z","shell.execute_reply":"2023-09-14T11:03:32.912916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(15, 10))\ni=0\nj=0\nfor index in comp.index:\n    if j>1:\n        i+=1\n        j=0\n    val = comp.loc[index, :].sort_values()\n    sns.barplot(x=val.values, y=val.index, ax=axs[i, j])\n    axs[i, j].set_title(f\"{index}\")\n    j+=1\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T11:03:32.915831Z","iopub.execute_input":"2023-09-14T11:03:32.916900Z","iopub.status.idle":"2023-09-14T11:03:33.907656Z","shell.execute_reply.started":"2023-09-14T11:03:32.916833Z","shell.execute_reply":"2023-09-14T11:03:33.906337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 1, figsize=(15, 30))\nfor i, model in enumerate([rfc_opt, lgbm_opt]):\n    fi = model.feature_importances_\n    df_fi = pd.DataFrame(fi, columns=['Feature importance'])\n    if model==rfc_opt:\n        df_fi['Feature name'] = model.feature_names_in_\n    else:\n        df_fi['Feature name'] = model.feature_name_\n    df_fi['Relative importance'] = round((df_fi['Feature importance']/np.max(df_fi['Feature importance']))*100, 2)\n    df_fi = df_fi[round(df_fi['Relative importance'], 2) > 5]\n    df_fi.sort_values(by='Relative importance', inplace=True)\n    \n    sns.barplot(data=df_fi, x='Relative importance', y='Feature name', orient='h', ax=axs[i])\n    for j in axs[i].containers:\n        axs[i].bar_label(j, )\n    axs[i].set_xlabel('Importance relative')\n    axs[i].set_ylabel('Variables')\n    \naxs[0].set_title(f'Importance relative des variabless\\nRandomForest')   \naxs[1].set_title(f'Importance relative des variabless\\nLGBMClassifier')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:13:28.751541Z","iopub.execute_input":"2023-09-15T11:13:28.751982Z","iopub.status.idle":"2023-09-15T11:13:31.353876Z","shell.execute_reply.started":"2023-09-15T11:13:28.751941Z","shell.execute_reply":"2023-09-15T11:13:31.352691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_fi = pd.DataFrame(rfc_opt.feature_importances_, columns=['Feature importance RFC'])\ndf_fi['Feature name RFC'] = rfc_opt.feature_names_in_\ndf_fi = df_fi.sort_values(by='Feature importance RFC', ascending=False).iloc[:len(predictive_var), :]\ndf_fi.reset_index(drop=True, inplace=True)\n\ndf_fi['predictive variables'] = predictive_var\n\ndf_fi2 = pd.DataFrame(lgbm_opt.feature_importances_, columns=['Feature importance LGBM'])\ndf_fi2['Feature name LGBM'] = lgbm_opt.feature_name_\ndf_fi2 = df_fi2.sort_values(by='Feature importance LGBM', ascending=False).iloc[:len(predictive_var), :]\ndf_fi2.reset_index(drop=True, inplace=True)\n\npd.concat([df_fi, df_fi2], axis=1)[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-15T11:13:31.355315Z","iopub.execute_input":"2023-09-15T11:13:31.355654Z","iopub.status.idle":"2023-09-15T11:13:31.386260Z","shell.execute_reply.started":"2023-09-15T11:13:31.355625Z","shell.execute_reply":"2023-09-15T11:13:31.385129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choix du modèle final : LGBMClassifier() avec over sampling","metadata":{}},{"cell_type":"markdown","source":"Il y a des features qui peuvent être supprimées:","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\n\nstart = time.time()\nrfecv = RFECV(estimator=LGBMClassifier(xgboost_dart_mode = False, uniform_drop = False, num_leaves = 200,\n                                       n_estimators=160, min_data_in_leaf = 1000, max_depth = 10), \n              step=5, \n              cv=StratifiedKFold(5), \n              scoring='roc_auc', \n              verbose=2,\n              n_jobs=-1)\nrfecv.fit(X_train_over_lgbm, y_train_over)\n\nprint('Time Elapsed: {}'.format(time.time()-start))\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)","metadata":{"execution":{"iopub.status.idle":"2023-09-15T12:48:59.893005Z","shell.execute_reply.started":"2023-09-15T11:14:59.367102Z","shell.execute_reply":"2023-09-15T12:48:59.890821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"step=5\nplt.figure(figsize=(20, 5))\nplt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\nplt.xlabel('Number of features tested (x%d)' % step)\nplt.ylabel('Cross-validation score (roc_auc)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:52:59.714128Z","iopub.execute_input":"2023-09-15T12:52:59.714587Z","iopub.status.idle":"2023-09-15T12:53:00.050753Z","shell.execute_reply.started":"2023-09-15T12:52:59.714553Z","shell.execute_reply":"2023-09-15T12:53:00.049354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranking = pd.DataFrame({'Features': rfecv.feature_names_in_})\nranking['RANK'] = np.asarray(rfecv.ranking_)\nranking.sort_values('RANK', inplace=True)\n\nfeatures_selection = ranking[ranking.RANK == 1]['Features'].to_list()\n\nX_train_over_lgbm = X_train_over_lgbm.loc[:, features_selection]\nX_val_lgbm = X_val_lgbm.loc[:, features_selection]\n\ndata_test = pd.read_csv('/kaggle/working/app_test.csv')\ndata_test_lgbm = data_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ndata_test = data_test_lgbm.loc[:, features_selection]\n\ndata_test['SK_ID_CURR'] = data_test_lgbm['SK_ID_CURR']\ndata_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:02:58.274574Z","iopub.execute_input":"2023-09-15T13:02:58.275069Z","iopub.status.idle":"2023-09-15T13:02:59.572657Z","shell.execute_reply.started":"2023-09-15T13:02:58.275033Z","shell.execute_reply":"2023-09-15T13:02:59.571527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_fin = LGBMClassifier(max_depth=10, min_data_in_leaf=1000, n_estimators=160,\n                          num_leaves=200, uniform_drop=False, xgboost_dart_mode=False)\nlgbm_fin.fit(X_train_over_lgbm, y_train_over)\n\ny_proba = lgbm_fin.predict_proba(X_val_lgbm)[:, 1]\nscores = [fbeta_score(y_val, pd.Series(y_proba).apply(lambda x: 1 if x > seuil else 0),\n                      beta=2) for seuil in np.linspace(0.1, 0.9, 17)]\nseuil_opt = np.linspace(0.1, 0.9, 17)[scores.index(np.max(scores))]\ny_pred = pd.Series(y_proba).apply(lambda x: 1 if x > seuil_opt else 0)\nprint(f\"Seuil optimal = {seuil_opt}\")\n    \nconf_mat = pd.DataFrame(confusion_matrix(y_val, y_pred), \n                            columns = ['pred_0', 'pred_1'],\n                            index = ['true_0', 'true_1'])\n     \nprecision = round(precision_score(y_val, y_pred), 3)\nrecall = round(recall_score(y_val, y_pred), 3)# Recall => priorité sur précision\nfbeta = round(fbeta_score(y_val, y_pred, beta=10), 3)# Coût FN = 10 * Coût FP\nauc = round(roc_auc_score(y_val, y_proba), 3)\n    \nprint(f\"Precision score       : {precision}\")\nprint(f\"Recall score          : {recall}\")\nprint(f\"FBêta score           : {fbeta}\")\nprint(f\"ROC AUC score         : {auc}\")\n    \ncomp['LGBM 49 features'] = [precision, recall, fbeta, auc]\nconf_mat","metadata":{"execution":{"iopub.status.idle":"2023-09-15T13:09:37.497387Z","shell.execute_reply.started":"2023-09-15T13:09:05.977377Z","shell.execute_reply":"2023-09-15T13:09:37.495882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp[['LGBM tuned', 'LGBM 49 features']]","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:10:05.943114Z","iopub.execute_input":"2023-09-15T13:10:05.943783Z","iopub.status.idle":"2023-09-15T13:10:05.955726Z","shell.execute_reply.started":"2023-09-15T13:10:05.943748Z","shell.execute_reply":"2023-09-15T13:10:05.954506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_selection","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:10:51.994462Z","iopub.execute_input":"2023-09-15T13:10:51.995477Z","iopub.status.idle":"2023-09-15T13:10:52.004683Z","shell.execute_reply.started":"2023-09-15T13:10:51.995427Z","shell.execute_reply":"2023-09-15T13:10:52.003404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **SHAP**","metadata":{}},{"cell_type":"markdown","source":"Vue globale : summary et dependence","metadata":{}},{"cell_type":"code","source":"import shap \n\ndata_test.reset_index(drop=True, inplace=True)\n\ndata = data_test.drop(columns=['SK_ID_CURR'])\n\nexplainer_lgbm = shap.TreeExplainer(lgbm_fin)\nshap_value_lgbm = explainer_lgbm.shap_values(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.summary_plot(shap_value_lgbm[0], data) # classe 0 : demande de prêt acceptée","metadata":{"execution":{"iopub.status.idle":"2023-09-15T13:14:57.585357Z","shell.execute_reply.started":"2023-09-15T13:14:47.155207Z","shell.execute_reply":"2023-09-15T13:14:57.584334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.dependence_plot(\"EXT_SOURCE_1\", shap_value_lgbm[0], data)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:16:59.234263Z","iopub.execute_input":"2023-09-15T13:16:59.234736Z","iopub.status.idle":"2023-09-15T13:17:01.240090Z","shell.execute_reply.started":"2023-09-15T13:16:59.234699Z","shell.execute_reply":"2023-09-15T13:17:01.239051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vue locale : force_plot","metadata":{}},{"cell_type":"code","source":"client_ID = 100005\nindex = data_test.loc[data_test.SK_ID_CURR==client_ID].index\npred = lgbm_fin.predict_proba(data.iloc[index, :])[:, 1]\nprint(pred)\n\nshap.plots.force(explainer_lgbm.expected_value[0], shap_value_lgbm[0][index,:], data.iloc[index, :], matplotlib = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:23:37.254725Z","iopub.execute_input":"2023-09-15T13:23:37.255248Z","iopub.status.idle":"2023-09-15T13:23:38.422042Z","shell.execute_reply.started":"2023-09-15T13:23:37.255210Z","shell.execute_reply":"2023-09-15T13:23:38.421224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exemple d'un prêt refusé.","metadata":{}},{"cell_type":"markdown","source":"Serialisations :","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open(r'/kaggle/working/LGBM_final.pkl', 'wb') as file:\n    pickle.dump(lgbm_fin, file)\nfile.close()\n\n# Modèle","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:32:27.195734Z","iopub.execute_input":"2023-09-15T13:32:27.196263Z","iopub.status.idle":"2023-09-15T13:32:27.285015Z","shell.execute_reply.started":"2023-09-15T13:32:27.196226Z","shell.execute_reply":"2023-09-15T13:32:27.283874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('shap_explainer_lgbm.p', 'wb') as f4:\n    pickle.dump(explainer_lgbm, f4)\n\nwith open('shap_values_lgbm.p', 'wb') as f4:\n    pickle.dump(shap_value_lgbm, f4)\n    \n# objets shap pour dashboard","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:35:10.596402Z","iopub.execute_input":"2023-09-15T13:35:10.597208Z","iopub.status.idle":"2023-09-15T13:35:10.799910Z","shell.execute_reply.started":"2023-09-15T13:35:10.597171Z","shell.execute_reply":"2023-09-15T13:35:10.798966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('applications.csv', 'wb') as f4:\n    pickle.dump(data_test, f4)\n    \n# demandes de prêts","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:37:38.665674Z","iopub.execute_input":"2023-09-15T13:37:38.666122Z","iopub.status.idle":"2023-09-15T13:37:38.695107Z","shell.execute_reply.started":"2023-09-15T13:37:38.666089Z","shell.execute_reply":"2023-09-15T13:37:38.694107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T13:37:39.238597Z","iopub.execute_input":"2023-09-15T13:37:39.238981Z","iopub.status.idle":"2023-09-15T13:37:39.247040Z","shell.execute_reply.started":"2023-09-15T13:37:39.238951Z","shell.execute_reply":"2023-09-15T13:37:39.245877Z"},"trusted":true},"execution_count":null,"outputs":[]}]}